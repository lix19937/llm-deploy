
采用了 PagedAttention，可以有效管理 attention 的 keys、values

State-of-the-art serving throughput     
Efficient management of attention key and value memory with PagedAttention    
Continuous batching of incoming requests   
Fast model execution with CUDA/HIP graph    
Quantization: GPTQ, AWQ, SqueezeLLM, FP8 KV Cache    
Optimized CUDA kernels    


https://zhuanlan.zhihu.com/p/642802585     
https://blog.vllm.ai/2023/06/20/vllm.html   
