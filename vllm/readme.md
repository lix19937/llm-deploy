
采用了 PagedAttention，可以有效管理 attention 的 keys、values

State-of-the-art serving throughput     
Efficient management of attention key and value memory with PagedAttention    
Continuous batching of incoming requests   
Fast model execution with CUDA/HIP graph    
Quantization: GPTQ, AWQ, SqueezeLLM, FP8 KV Cache    
Optimized CUDA kernels    

